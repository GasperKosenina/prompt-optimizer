{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# When Does Prompt Optimization Matter?\n",
    "\n",
    "## An Empirical Study of DSPy Prompt Optimization Across Task Types\n",
    "\n",
    "---\n",
    "\n",
    "**Research Question:** Does automated prompt optimization provide meaningful improvements across different task types, or are modern LLMs already well-suited for certain tasks out of the box?\n",
    "\n",
    "**Hypothesis:** The benefit of prompt optimization varies by task complexity:\n",
    "- **Simple classification tasks** may see minimal improvement (LLMs already perform well)\n",
    "- **Complex reasoning tasks** may see significant improvement (optimization discovers effective strategies)\n",
    "\n",
    "**Framework:** [DSPy](https://github.com/stanfordnlp/dspy) - A framework for programming with foundation models that replaces manual prompt engineering with automatic optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dspy-overview",
   "metadata": {},
   "source": [
    "### DSPy Overview\n",
    "\n",
    "DSPy (Declarative Self-improving Language Programs) provides:\n",
    "\n",
    "1. **Signatures** - Declarative specifications of input/output behavior\n",
    "2. **Modules** - Composable building blocks (e.g., `ChainOfThought`)\n",
    "3. **Optimizers** - Automatic prompt optimization algorithms\n",
    "\n",
    "```python\n",
    "# Example: Define a signature\n",
    "class IntentClassifier(dspy.Signature):\n",
    "    \"\"\"Classify customer intent from support query.\"\"\"\n",
    "    query: str = dspy.InputField()\n",
    "    intent: str = dspy.OutputField()\n",
    "\n",
    "# Create module and optimize\n",
    "classifier = dspy.Predict(IntentClassifier)\n",
    "optimized = optimizer.compile(classifier, trainset=data)\n",
    "```\n",
    "\n",
    "**Key Insight:** In DSPy, the signature docstring and field descriptions become the \"system prompt\". Optimizers like MIPROv2 can discover better instructions and few-shot examples automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Configure plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"font.size\"] = 11\n",
    "\n",
    "# Define paths\n",
    "RESULTS_DIR = project_root / \"results\"\n",
    "VIZ_DIR = RESULTS_DIR / \"visualizations\"\n",
    "DATASETS_DIR = project_root / \"datasets\"\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Visualizations: {VIZ_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-all-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all pre-computed results\n",
    "def load_json(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Intent Classification\n",
    "intent_results = load_json(RESULTS_DIR / \"intent_classification\" / \"optimization.json\")\n",
    "intent_model = load_json(RESULTS_DIR / \"intent_classification\" / \"optimized_model.json\")\n",
    "\n",
    "# Response Generation\n",
    "response_results = load_json(RESULTS_DIR / \"response_generation\" / \"optimization.json\")\n",
    "response_model = load_json(RESULTS_DIR / \"response_generation\" / \"optimized_model.json\")\n",
    "\n",
    "# Math Solver\n",
    "math_light_results = load_json(RESULTS_DIR / \"math_solver\" / \"light_optimization.json\")\n",
    "math_medium_results = load_json(RESULTS_DIR / \"math_solver\" / \"medium_optimization.json\")\n",
    "math_model = load_json(RESULTS_DIR / \"math_solver\" / \"optimized_model_light.json\")\n",
    "\n",
    "print(\"All results loaded successfully!\")\n",
    "print(f\"\\nExperiments:\")\n",
    "print(f\"  - Intent Classification: {intent_results['optimizer']}\")\n",
    "print(f\"  - Response Generation: MIPROv2\")\n",
    "print(f\"  - Math Solver: MIPROv2 (Light & Medium)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "methodology-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Methodology\n",
    "\n",
    "### Task Selection Rationale\n",
    "\n",
    "We selected three tasks representing a spectrum of complexity:\n",
    "\n",
    "| Task | Type | Complexity | Expected Optimization Benefit |\n",
    "|------|------|------------|------------------------------|\n",
    "| Intent Classification | Pattern Matching | Low | Minimal (LLMs already good) |\n",
    "| Response Generation | Text Generation | Medium | Moderate (benefits from instructions) |\n",
    "| Math Word Problems | Multi-step Reasoning | High | Significant (benefits from strategies) |\n",
    "\n",
    "### Optimizers Used\n",
    "\n",
    "1. **BootstrapFewShot**\n",
    "   - Selects effective few-shot examples from training data\n",
    "   - Fast and low-cost\n",
    "   - Best for: Simple tasks where examples help\n",
    "\n",
    "2. **MIPROv2 (Multi-prompt Instruction PRoposal Optimizer)**\n",
    "   - Uses meta-prompting to generate instruction candidates\n",
    "   - Evaluates and selects best instructions + demos\n",
    "   - Modes: Light (fewer trials), Medium (more thorough)\n",
    "   - Best for: Complex tasks where instructions matter\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "- **Intent Classification:** Exact match accuracy\n",
    "- **Response Generation:** LLM-as-Judge quality score (0.0-1.0)\n",
    "- **Math Solver:** Numerical answer accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "datasets-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Dataset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bitext-intro",
   "metadata": {},
   "source": [
    "### Dataset 1: Bitext Customer Support (Intent Classification & Response Generation)\n",
    "\n",
    "The Bitext dataset contains 27,000 customer support interactions across 27 intent categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-bitext",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Bitext dataset\n",
    "bitext_path = DATASETS_DIR / \"Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv\"\n",
    "bitext_df = pd.read_csv(bitext_path)\n",
    "\n",
    "print(f\"Dataset size: {len(bitext_df):,} examples\")\n",
    "print(f\"Intent categories: {bitext_df['intent'].nunique()}\")\n",
    "print(f\"High-level categories: {bitext_df['category'].nunique()}\")\n",
    "print(f\"\\nColumns: {list(bitext_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bitext-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample examples\n",
    "print(\"Sample Customer Support Queries:\\n\")\n",
    "samples = bitext_df.groupby('intent').first().reset_index().head(5)\n",
    "for _, row in samples.iterrows():\n",
    "    print(f\"Intent: {row['intent']}\")\n",
    "    print(f\"Query: {row['instruction'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bitext-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot intent distribution\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "intent_counts = bitext_df['intent'].value_counts()\n",
    "colors = plt.cm.viridis(np.linspace(0, 0.8, len(intent_counts)))\n",
    "\n",
    "bars = ax.bar(range(len(intent_counts)), intent_counts.values, color=colors)\n",
    "ax.set_xticks(range(len(intent_counts)))\n",
    "ax.set_xticklabels(intent_counts.index, rotation=45, ha='right', fontsize=9)\n",
    "ax.set_xlabel('Intent Category', fontweight='bold')\n",
    "ax.set_ylabel('Number of Examples', fontweight='bold')\n",
    "ax.set_title('Bitext Dataset: Distribution of Intent Categories', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Add count labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}', ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDataset is balanced: each intent has ~{intent_counts.mean():.0f} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gsm8k-intro",
   "metadata": {},
   "source": [
    "### Dataset 2: GSM8K (Math Word Problems)\n",
    "\n",
    "GSM8K contains grade-school level math word problems requiring multi-step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-gsm8k",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GSM8K samples\n",
    "gsm8k_train = []\n",
    "with open(DATASETS_DIR / \"train.jsonl\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 5:  # Just load a few for display\n",
    "            break\n",
    "        gsm8k_train.append(json.loads(line))\n",
    "\n",
    "# Count total\n",
    "with open(DATASETS_DIR / \"train.jsonl\") as f:\n",
    "    train_count = sum(1 for _ in f)\n",
    "with open(DATASETS_DIR / \"test.jsonl\") as f:\n",
    "    test_count = sum(1 for _ in f)\n",
    "\n",
    "print(f\"GSM8K Dataset:\")\n",
    "print(f\"  Training examples: {train_count:,}\")\n",
    "print(f\"  Test examples: {test_count:,}\")\n",
    "print(f\"  Total: {train_count + test_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gsm8k-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample problems\n",
    "print(\"Sample Math Word Problems:\\n\")\n",
    "print(\"=\" * 80)\n",
    "for i, item in enumerate(gsm8k_train[:3], 1):\n",
    "    print(f\"\\nProblem {i}:\")\n",
    "    print(f\"{item['question']}\")\n",
    "    # Extract just the final answer\n",
    "    answer = item['answer'].split('####')[-1].strip() if '####' in item['answer'] else item['answer']\n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment1-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Experiment 1 - Intent Classification\n",
    "\n",
    "**Task:** Classify customer support queries into one of 27 intent categories.\n",
    "\n",
    "**Hypothesis:** Modern LLMs should already be good at this pattern-matching task, yielding minimal improvement from optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intent-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display intent classification results\n",
    "print(\"Intent Classification Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nOptimizer: {intent_results['optimizer']}\")\n",
    "print(f\"Dataset: {intent_results['dataset']}\")\n",
    "print(f\"Train size: {intent_results['dataset_sizes']['train']}\")\n",
    "print(f\"Test size: {intent_results['dataset_sizes']['test']}\")\n",
    "print()\n",
    "print(f\"Baseline Accuracy:  {intent_results['baseline']['accuracy']:.1f}%\")\n",
    "print(f\"Optimized Accuracy: {intent_results['optimized']['accuracy']:.1f}%\")\n",
    "print(f\"\\nChange: {intent_results['improvement']['absolute']:+.1f}% ({intent_results['improvement']['percentage']:+.1f}% relative)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intent-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize intent classification results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Bar chart comparison\n",
    "categories = ['Baseline', 'Optimized']\n",
    "accuracies = [intent_results['baseline']['accuracy'], intent_results['optimized']['accuracy']]\n",
    "colors = ['#3498db', '#e74c3c']  # Blue for baseline, red for worse\n",
    "\n",
    "bars = ax1.bar(categories, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "ax1.set_ylabel('Accuracy (%)', fontweight='bold')\n",
    "ax1.set_title('Intent Classification: Baseline vs Optimized', fontweight='bold')\n",
    "ax1.set_ylim([80, 100])\n",
    "ax1.axhline(y=90, color='gray', linestyle='--', alpha=0.5, label='90% threshold')\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "            f'{height:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Improvement delta\n",
    "improvement = intent_results['improvement']['absolute']\n",
    "color = '#2ecc71' if improvement > 0 else '#e74c3c'\n",
    "ax2.bar(['BootstrapFewShot'], [improvement], color=color, alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "ax2.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "ax2.set_ylabel('Accuracy Change (%)', fontweight='bold')\n",
    "ax2.set_title('Optimization Impact', fontweight='bold')\n",
    "ax2.set_ylim([-5, 5])\n",
    "ax2.text(0, improvement - 0.3, f'{improvement:+.1f}%', ha='center', va='top', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intent-analysis",
   "metadata": {},
   "source": [
    "### Analysis: Intent Classification\n",
    "\n",
    "**Key Finding:** The baseline model already achieves 92% accuracy, and optimization actually resulted in a slight decrease (-2%).\n",
    "\n",
    "**Interpretation:**\n",
    "- Modern LLMs are already well-suited for intent classification\n",
    "- The task involves pattern matching against a fixed set of categories\n",
    "- Few-shot examples may have introduced noise rather than helping\n",
    "- The high baseline leaves little room for improvement\n",
    "\n",
    "**Conclusion:** For simple classification tasks where LLMs already perform well, prompt optimization provides minimal or no benefit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment2-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Experiment 2 - Response Generation\n",
    "\n",
    "**Task:** Generate helpful customer support responses given a query and detected intent.\n",
    "\n",
    "**Evaluation:** LLM-as-Judge (GPT-4o-mini rates response quality 0.0-1.0)\n",
    "\n",
    "**Hypothesis:** Response quality should improve with better instructions discovered by MIPROv2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "response-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display response generation results\n",
    "print(\"Response Generation Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nOptimizer: MIPROv2\")\n",
    "print(f\"Train size: {response_results['dataset']['train_size']}\")\n",
    "print(f\"Test size: {response_results['dataset']['test_size']}\")\n",
    "print()\n",
    "print(\"Baseline:\")\n",
    "print(f\"  Average Quality: {response_results['baseline']['average_quality']:.3f}\")\n",
    "print(f\"  Min/Max: {response_results['baseline']['min_quality']:.2f} / {response_results['baseline']['max_quality']:.2f}\")\n",
    "print()\n",
    "print(\"Optimized:\")\n",
    "print(f\"  Average Quality: {response_results['optimized']['average_quality']:.3f}\")\n",
    "print(f\"  Min/Max: {response_results['optimized']['min_quality']:.2f} / {response_results['optimized']['max_quality']:.2f}\")\n",
    "print()\n",
    "print(f\"Improvement: +{response_results['improvement']['absolute']:.3f} ({response_results['improvement']['percentage']:+.1f}% relative)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "response-instructions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare instructions before and after optimization\n",
    "print(\"Discovered Instructions (Before vs After Optimization)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"BASELINE INSTRUCTION:\")\n",
    "print(\"-\" * 70)\n",
    "print(response_results['baseline']['instructions'])\n",
    "print()\n",
    "print(\"OPTIMIZED INSTRUCTION (discovered by MIPROv2):\")\n",
    "print(\"-\" * 70)\n",
    "print(response_results['optimized']['instructions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "response-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize response generation results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Quality comparison\n",
    "categories = ['Baseline', 'Optimized']\n",
    "qualities = [response_results['baseline']['average_quality'], \n",
    "             response_results['optimized']['average_quality']]\n",
    "colors = ['#3498db', '#2ecc71']\n",
    "\n",
    "bars = ax1.bar(categories, qualities, color=colors, alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "ax1.set_ylabel('Average Quality Score', fontweight='bold')\n",
    "ax1.set_title('Response Generation: Quality Comparison', fontweight='bold')\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "            f'{height:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Quality distribution (min/avg/max)\n",
    "x = np.arange(2)\n",
    "width = 0.25\n",
    "\n",
    "mins = [response_results['baseline']['min_quality'], response_results['optimized']['min_quality']]\n",
    "avgs = [response_results['baseline']['average_quality'], response_results['optimized']['average_quality']]\n",
    "maxs = [response_results['baseline']['max_quality'], response_results['optimized']['max_quality']]\n",
    "\n",
    "ax2.bar(x - width, mins, width, label='Min', color='#e74c3c', alpha=0.8)\n",
    "ax2.bar(x, avgs, width, label='Average', color='#3498db', alpha=0.8)\n",
    "ax2.bar(x + width, maxs, width, label='Max', color='#2ecc71', alpha=0.8)\n",
    "\n",
    "ax2.set_ylabel('Quality Score', fontweight='bold')\n",
    "ax2.set_title('Quality Distribution: Baseline vs Optimized', fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(['Baseline', 'Optimized'])\n",
    "ax2.legend()\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "response-analysis",
   "metadata": {},
   "source": [
    "### Analysis: Response Generation\n",
    "\n",
    "**Key Finding:** MIPROv2 discovered instructions that improved average quality from 0.713 to 0.735 (+3.1%).\n",
    "\n",
    "**Notable Improvements:**\n",
    "- Minimum quality increased from 0.40 to 0.60 (fewer bad responses)\n",
    "- Maximum quality increased from 0.92 to 0.95\n",
    "- The discovered instruction is more specific about empathy and structure\n",
    "\n",
    "**The Optimized Instruction:**\n",
    "> \"Given a customer support query and the detected customer intent, use the Predict module to predict the reasoning behind the query and generate a tailored response that aligns with both the query and intent. Ensure that the response is professional, empathetic, and provides clear next steps for the customer.\"\n",
    "\n",
    "**Conclusion:** For generation tasks, optimization provides moderate improvement by discovering more effective instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment3-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Experiment 3 - Math Word Problems\n",
    "\n",
    "**Task:** Solve grade-school math word problems requiring multi-step reasoning.\n",
    "\n",
    "**Dataset:** GSM8K (Grade School Math 8K)\n",
    "\n",
    "**Hypothesis:** Complex reasoning tasks should benefit significantly from optimization (discovered strategies + few-shot examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "math-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display math solver results\n",
    "print(\"Math Word Problem Solver Results\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(f\"Baseline Accuracy: {math_light_results['baseline']['accuracy']:.1f}%\")\n",
    "print(f\"  Correct: {math_light_results['baseline']['correct']}/50\")\n",
    "print()\n",
    "print(\"MIPROv2 Light:\")\n",
    "print(f\"  Accuracy: {math_light_results['optimized']['accuracy']:.1f}%\")\n",
    "print(f\"  Correct: {math_light_results['optimized']['correct']}/50\")\n",
    "print(f\"  Improvement: +{math_light_results['improvement']['absolute']:.1f}%\")\n",
    "print()\n",
    "print(\"MIPROv2 Medium:\")\n",
    "opt_medium = math_medium_results['optimizations'][0]\n",
    "print(f\"  Accuracy: {opt_medium['accuracy']:.1f}%\")\n",
    "print(f\"  Correct: {opt_medium['correct']}/50\")\n",
    "print(f\"  Improvement: +{opt_medium['accuracy'] - math_medium_results['baseline']['accuracy']:.1f}%\")\n",
    "print(f\"  Optimization time: {opt_medium['time_seconds']/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "math-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize math solver results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "categories = ['Baseline', 'MIPROv2 Light', 'MIPROv2 Medium']\n",
    "accuracies = [\n",
    "    math_light_results['baseline']['accuracy'],\n",
    "    math_light_results['optimized']['accuracy'],\n",
    "    math_medium_results['optimizations'][0]['accuracy']\n",
    "]\n",
    "colors = ['#3498db', '#f39c12', '#2ecc71']\n",
    "\n",
    "bars = ax1.bar(categories, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "ax1.set_ylabel('Accuracy (%)', fontweight='bold')\n",
    "ax1.set_title('Math Solver: Accuracy by Optimization Level', fontweight='bold')\n",
    "ax1.set_ylim([0, 100])\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "            f'{height:.0f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Improvement delta\n",
    "improvements = [\n",
    "    math_light_results['improvement']['absolute'],\n",
    "    math_medium_results['optimizations'][0]['accuracy'] - math_medium_results['baseline']['accuracy']\n",
    "]\n",
    "opt_names = ['MIPROv2 Light', 'MIPROv2 Medium']\n",
    "\n",
    "bars2 = ax2.bar(opt_names, improvements, color=['#f39c12', '#2ecc71'], alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "ax2.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "ax2.set_ylabel('Accuracy Improvement (%)', fontweight='bold')\n",
    "ax2.set_title('Optimization Impact: Light vs Medium', fontweight='bold')\n",
    "ax2.set_ylim([0, 30])\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "            f'+{height:.0f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "math-demos",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show discovered few-shot examples\n",
    "print(\"Few-Shot Examples Discovered by MIPROv2\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"These examples were automatically selected to demonstrate effective reasoning:\")\n",
    "print()\n",
    "\n",
    "demos = math_model['predict']['demos']\n",
    "for i, demo in enumerate(demos[:2], 1):  # Show first 2\n",
    "    print(f\"EXAMPLE {i}:\")\n",
    "    print(f\"Question: {demo['question']}\")\n",
    "    print(f\"\\nReasoning: {demo['reasoning']}\")\n",
    "    print(f\"\\nAnswer: {demo['answer']}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "math-analysis",
   "metadata": {},
   "source": [
    "### Analysis: Math Word Problems\n",
    "\n",
    "**Key Finding:** MIPROv2 achieved dramatic improvements:\n",
    "- Light mode: 64% → 84% (+20% absolute, +31% relative)\n",
    "- Medium mode: 64% → 88% (+24% absolute, +38% relative)\n",
    "\n",
    "**Why Such Large Improvements?**\n",
    "\n",
    "1. **Few-shot examples matter**: The discovered examples demonstrate clear step-by-step reasoning\n",
    "2. **Task requires strategy**: Multi-step arithmetic benefits from explicit decomposition\n",
    "3. **Low baseline leaves room**: 64% baseline indicates significant room for improvement\n",
    "4. **Chain-of-thought prompting**: The optimized model uses structured reasoning\n",
    "\n",
    "**Conclusion:** Complex reasoning tasks benefit substantially from prompt optimization, with gains of 20-24% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Cross-Task Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = {\n",
    "    'Task': ['Intent Classification', 'Response Generation', 'Math Solver (Light)', 'Math Solver (Medium)'],\n",
    "    'Baseline': [92.0, 71.3, 64.0, 64.0],\n",
    "    'Optimized': [90.0, 73.5, 84.0, 88.0],\n",
    "    'Change (abs)': [-2.0, 2.2, 20.0, 24.0],\n",
    "    'Change (%)': [-2.2, 3.1, 31.3, 37.5],\n",
    "    'Optimizer': ['BootstrapFewShot', 'MIPROv2', 'MIPROv2 Light', 'MIPROv2 Medium']\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"Summary of All Experiments\")\n",
    "print(\"=\" * 90)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Baseline vs Optimized\n",
    "ax1 = axes[0]\n",
    "x = np.arange(4)\n",
    "width = 0.35\n",
    "\n",
    "baseline = [92.0, 71.3, 64.0, 64.0]\n",
    "optimized = [90.0, 73.5, 84.0, 88.0]\n",
    "tasks = ['Intent\\nClassification', 'Response\\nGeneration', 'Math\\n(Light)', 'Math\\n(Medium)']\n",
    "\n",
    "ax1.bar(x - width/2, baseline, width, label='Baseline', color='#3498db', alpha=0.8)\n",
    "ax1.bar(x + width/2, optimized, width, label='Optimized', color='#2ecc71', alpha=0.8)\n",
    "ax1.set_ylabel('Score (%)', fontweight='bold')\n",
    "ax1.set_title('Performance: Baseline vs Optimized', fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(tasks)\n",
    "ax1.legend()\n",
    "ax1.set_ylim([0, 100])\n",
    "\n",
    "# Plot 2: Improvement delta\n",
    "ax2 = axes[1]\n",
    "improvements = [-2.0, 2.2, 20.0, 24.0]\n",
    "colors = ['#e74c3c', '#2ecc71', '#2ecc71', '#2ecc71']\n",
    "\n",
    "bars = ax2.bar(tasks, improvements, color=colors, alpha=0.8, edgecolor='black', linewidth=1)\n",
    "ax2.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "ax2.set_ylabel('Improvement (%)', fontweight='bold')\n",
    "ax2.set_title('Optimization Impact by Task', fontweight='bold')\n",
    "\n",
    "for bar, imp in zip(bars, improvements):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + (1 if height >= 0 else -2),\n",
    "            f'{imp:+.0f}%', ha='center', va='bottom' if height >= 0 else 'top', \n",
    "            fontsize=11, fontweight='bold')\n",
    "\n",
    "# Plot 3: Task complexity vs benefit\n",
    "ax3 = axes[2]\n",
    "complexity = [1, 2, 3, 3]  # Arbitrary complexity scores\n",
    "benefit = [-2.0, 2.2, 20.0, 24.0]\n",
    "task_labels = ['Intent', 'Response', 'Math (L)', 'Math (M)']\n",
    "colors = plt.cm.RdYlGn(np.interp(benefit, [-5, 25], [0, 1]))\n",
    "\n",
    "scatter = ax3.scatter(complexity, benefit, s=300, c=colors, edgecolors='black', linewidth=1.5)\n",
    "for i, label in enumerate(task_labels):\n",
    "    ax3.annotate(label, (complexity[i], benefit[i]), textcoords=\"offset points\", \n",
    "                 xytext=(0, 12), ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax3.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax3.set_xlabel('Task Complexity', fontweight='bold')\n",
    "ax3.set_ylabel('Optimization Benefit (%)', fontweight='bold')\n",
    "ax3.set_title('Complexity vs Optimization Benefit', fontweight='bold')\n",
    "ax3.set_xticks([1, 2, 3])\n",
    "ax3.set_xticklabels(['Low\\n(Classification)', 'Medium\\n(Generation)', 'High\\n(Reasoning)'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display existing visualization (from intent classification experiment)\n",
    "improvement_path = VIZ_DIR / \"improvement_delta.png\"\n",
    "if improvement_path.exists():\n",
    "    print(\"Pre-generated Visualization: Intent Classification Improvement\")\n",
    "    display(Image(filename=str(improvement_path), width=600))\n",
    "else:\n",
    "    print(f\"Note: Visualization not found at {improvement_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Conclusions\n",
    "\n",
    "### Key Finding: Task Complexity Determines Optimization Value\n",
    "\n",
    "Our experiments reveal a clear pattern:\n",
    "\n",
    "| Task Complexity | Example | Optimization Benefit | Recommendation |\n",
    "|----------------|---------|---------------------|----------------|\n",
    "| **Low** | Intent Classification | Minimal/Negative (-2%) | Skip optimization; use baseline |\n",
    "| **Medium** | Response Generation | Moderate (+3%) | Optional; cost-benefit analysis needed |\n",
    "| **High** | Math Reasoning | Significant (+20-24%) | Strongly recommended |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-section",
   "metadata": {},
   "source": [
    "### Why Does This Pattern Emerge?\n",
    "\n",
    "**Low-Complexity Tasks (Classification):**\n",
    "- Modern LLMs have strong pattern-matching capabilities\n",
    "- The mapping from input to output is relatively straightforward\n",
    "- Baseline performance is already high (92%)\n",
    "- Few-shot examples may introduce noise rather than help\n",
    "\n",
    "**Medium-Complexity Tasks (Generation):**\n",
    "- Quality depends on following implicit guidelines\n",
    "- Discovered instructions can codify better practices\n",
    "- Improvement comes from clearer task specification\n",
    "\n",
    "**High-Complexity Tasks (Reasoning):**\n",
    "- Success requires structured problem decomposition\n",
    "- Few-shot examples demonstrate effective strategies\n",
    "- Chain-of-thought reasoning significantly helps\n",
    "- Low baseline indicates room for improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recommendations",
   "metadata": {},
   "source": [
    "### Practical Recommendations\n",
    "\n",
    "**When to Use Prompt Optimization:**\n",
    "\n",
    "1. **Always optimize** when:\n",
    "   - Task requires multi-step reasoning\n",
    "   - Baseline performance is below 70%\n",
    "   - Task has complex output structure\n",
    "\n",
    "2. **Consider optimizing** when:\n",
    "   - Quality consistency is important\n",
    "   - You have training data and compute budget\n",
    "   - Small improvements justify the cost\n",
    "\n",
    "3. **Skip optimization** when:\n",
    "   - Baseline is already >90%\n",
    "   - Task is simple classification\n",
    "   - Compute budget is limited\n",
    "\n",
    "**Optimizer Selection:**\n",
    "- **BootstrapFewShot**: Fast, cheap, good for simple improvements\n",
    "- **MIPROv2 Light**: Moderate cost, discovers instructions\n",
    "- **MIPROv2 Medium**: Higher cost, better for complex tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limitations",
   "metadata": {},
   "source": [
    "### Limitations and Future Work\n",
    "\n",
    "**Limitations:**\n",
    "- Small test sets (50 examples) may have variance\n",
    "- Single model (GPT-3.5-turbo) - results may differ with other models\n",
    "- Limited optimizer configurations tested\n",
    "- LLM-as-Judge evaluation has its own biases\n",
    "\n",
    "**Future Directions:**\n",
    "- Test with larger evaluation sets\n",
    "- Compare across multiple LLM backends\n",
    "- Explore more task types (summarization, translation, coding)\n",
    "- Investigate optimizer hyperparameter sensitivity\n",
    "- Cost-benefit analysis including optimization compute costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This research demonstrates that **prompt optimization is not universally beneficial**. The value of optimization depends critically on task complexity:\n",
    "\n",
    "- **Simple tasks** (classification): LLMs already perform well; optimization unnecessary\n",
    "- **Complex tasks** (multi-step reasoning): Optimization provides substantial gains (20%+)\n",
    "\n",
    "**The key insight:** Before investing in prompt optimization, evaluate your baseline performance. If it's already high (>90%), focus efforts elsewhere. If it's moderate or low, especially for reasoning-heavy tasks, optimization can yield significant improvements.\n",
    "\n",
    "---\n",
    "\n",
    "*Research conducted using [DSPy](https://github.com/stanfordnlp/dspy) framework with GPT-3.5-turbo as the base model.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
